
# Maintenance of Ubuntu Linux Machine for High Availability and Disaster Prevention

Maintaining an Ubuntu Linux machine for **high availability (HA)** and **disaster prevention** involves proactive monitoring, resource optimization, security hardening, and automation to prevent downtime, errors, CPU/memory/disk throttling, and resource exhaustion. As a Linux administrator and DevOps engineer deploying large applications, your goal is to ensure the system remains stable, secure, and scalable. This note outlines daily tasks, standard practices, and Python-based automation scripts to achieve these objectives, tailored for enterprise environments hosting mission-critical applications (e.g., web servers, databases, containerized apps).

## Key Objectives
- **Prevent Downtime**: Ensure services (e.g., Nginx, Docker) are always available.
- **Avoid Resource Throttling**: Monitor and manage CPU, memory, disk, and network usage.
- **Mitigate Disasters**: Implement backups, logging, and recovery plans.
- **Maintain Efficiency**: Optimize system performance and automate repetitive tasks.
- **Ensure Security**: Apply patches, restrict access, and monitor for threats.
- **Support High Availability**: Use redundancy, monitoring, and failover strategies.

## Core Maintenance Components
1. **System Monitoring**: Track CPU, memory, disk, and network usage.
2. **Service Management**: Ensure critical services (e.g., Nginx, Docker) are running.
3. **Backup and Recovery**: Automate backups and test restores (from your backup topic).
4. **Security Hardening**: Apply updates, manage firewalls, and monitor logs.
5. **Automation**: Use Python scripts for monitoring, cleanup, and alerting.
6. **High Availability**: Configure redundancy and failover for large applications.
7. **Log Management**: Rotate and analyze logs to prevent disk issues (from your logging topic).

## Daily Tasks and Responsibilities
As a Linux administrator and DevOps engineer, your daily tasks focus on proactive maintenance and rapid response to issues. Below are the key responsibilities, organized by category, with practical examples and Python scripts where applicable.

### 1. System Monitoring
**Goal**: Prevent CPU, memory, disk, or network throttling by monitoring resource usage.
- **Tasks**:
  - Check CPU/memory usage: Use `htop` or `top` to identify high-usage processes.
  - Monitor disk usage: Ensure `/var`, `/var/lib/docker` donâ€™t fill up.
  - Track network activity: Verify bandwidth usage for large apps.
  - Use automated monitoring tools: Integrate with Prometheus/Grafana (from your monitoring topic).
- **Commands/Tools**:
  - `htop`: View process usage (`sudo apt install htop`).
  - `df -h`: Check disk usage.
  - `iotop`: Monitor disk I/O (`sudo apt install iotop`).
  - `nload`: Monitor network usage (`sudo apt install nload`).
- **Example**: `df -h /var/lib/docker` to check Docker storage usage.
- **Automation**: Python script to monitor resources and alert via email if thresholds are exceeded.

```x-python
import psutil
import smtplib
from email.mime.text import MIMEText
import datetime
import logging

# Configuration
LOG_FILE = "/var/log/resource_monitor.log"
EMAIL = "admin@example.com"
SMTP_SERVER = "smtp.example.com"
SMTP_PORT = 587
SMTP_USER = "your_smtp_user"
SMTP_PASS = "your_smtp_password"
CPU_THRESHOLD = 80  # % CPU usage
MEM_THRESHOLD = 80  # % Memory usage
DISK_THRESHOLD = 80  # % Disk usage for /var/lib/docker

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")

def send_alert(subject, message):
    """Send email alert."""
    msg = MIMEText(message)
    msg["Subject"] = subject
    msg["From"] = EMAIL
    msg["To"] = EMAIL
    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASS)
            server.sendmail(EMAIL, EMAIL, msg.as_string())
        logging.info(f"Alert sent: {subject}")
    except Exception as e:
        logging.error(f"Failed to send alert: {e}")

def monitor_resources():
    """Monitor CPU, memory, and disk usage."""
    # CPU usage
    cpu_percent = psutil.cpu_percent(interval=1)
    if cpu_percent > CPU_THRESHOLD:
        send_alert("High CPU Usage Alert", f"CPU usage is {cpu_percent}% on {datetime.datetime.now()}")

    # Memory usage
    mem = psutil.virtual_memory()
    mem_percent = mem.percent
    if mem_percent > MEM_THRESHOLD:
        send_alert("High Memory Usage Alert", f"Memory usage is {mem_percent}% on {datetime.datetime.now()}")

    # Disk usage for /var/lib/docker
    disk = psutil.disk_usage("/var/lib/docker")
    disk_percent = disk.percent
    if disk_percent > DISK_THRESHOLD:
        send_alert("High Disk Usage Alert", f"Disk usage is {disk_percent}% on /var/lib/docker")

    logging.info(f"CPU: {cpu_percent}%, Memory: {mem_percent}%, Disk: {disk_percent}%")

if __name__ == "__main__":
    monitor_resources()
```

- **Script Explanation**:
  - Uses `psutil` to monitor CPU, memory, and disk usage (`sudo pip install psutil`).
  - Sends email alerts if thresholds (80%) are exceeded, using `smtplib` (configure SMTP server, e.g., Gmail).
  - Logs results to `/var/log/resource_monitor.log` for auditing.
  - **Execution**: `sudo python3 /usr/local/bin/monitor_resources.py`.
  - **Schedule**: Add to `crontab -e`: `*/15 * * * * python3 /usr/local/bin/monitor_resources.py` (runs every 15 minutes).
- **Enterprise Impact**: Prevents resource throttling, aligns with your monitoring topic, and integrates with Prometheus (e.g., export metrics via `prometheus_client`).

### 2. Service Management
**Goal**: Ensure critical services (e.g., Nginx, Docker, application-specific services) are running to avoid downtime.
- **Tasks**:
  - Check service status: Verify services like `nginx`, `docker`, and `ssh` are active.
  - Restart failed services: Automatically restart if stopped.
  - Monitor containerized apps: Check Docker container health (from your containerization topic).
- **Commands/Tools**:
  - `systemctl status <service>`: Check service status (e.g., `systemctl status nginx`).
  - `systemctl restart <service>`: Restart a service.
  - `docker ps`: List running containers.
- **Example**: `systemctl is-active nginx || systemctl restart nginx` to check and restart Nginx if stopped.
- **Automation**: Python script to monitor and restart services.

```x-python
import subprocess
import logging
import smtplib
from email.mime.text import MIMEText
import datetime

# Configuration
LOG_FILE = "/var/log/service_monitor.log"
EMAIL = "admin@example.com"
SMTP_SERVER = "smtp.example.com"
SMTP_PORT = 587
SMTP_USER = "your_smtp_user"
SMTP_PASS = "your_smtp_password"
SERVICES = ["nginx", "docker"]

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")

def send_alert(subject, message):
    """Send email alert."""
    msg = MIMEText(message)
    msg["Subject"] = subject
    msg["From"] = EMAIL
    msg["To"] = EMAIL
    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASS)
            server.sendmail(EMAIL, EMAIL, msg.as_string())
        logging.info(f"Alert sent: {subject}")
    except Exception as e:
        logging.error(f"Failed to send alert: {e}")

def monitor_services():
    """Monitor and restart services if stopped."""
    for service in SERVICES:
        try:
            result = subprocess.run(["systemctl", "is-active", "--quiet", service], check=True)
            logging.info(f"Service {service} is active")
        except subprocess.CalledProcessError:
            logging.warning(f"Service {service} is stopped, attempting restart")
            try:
                subprocess.run(["systemctl", "restart", service], check=True)
                send_alert(f"Service {service} Restarted", f"Service {service} was stopped and restarted on {datetime.datetime.now()}")
                logging.info(f"Service {service} restarted successfully")
            except subprocess.CalledProcessError as e:
                send_alert(f"Service {service} Restart Failed", f"Failed to restart {service}: {e}")
                logging.error(f"Failed to restart {service}: {e}")

if __name__ == "__main__":
    monitor_services()
```

- **Script Explanation**:
  - Uses `subprocess` to check and restart services (`nginx`, `docker`).
  - Sends email alerts if a service is stopped and restarted or fails to restart.
  - Logs actions to `/var/log/service_monitor.log`.
  - **Execution**: `sudo python3 /usr/local/bin/monitor_services.py`.
  - **Schedule**: Add to `crontab -e`: `*/10 * * * * python3 /usr/local/bin/monitor_services.py` (runs every 10 minutes).
- **Enterprise Impact**: Ensures high availability by keeping critical services running, aligns with your containerization topic for Docker services.

### 3. Backup and Recovery
**Goal**: Protect data to prevent loss from disasters (from your backup topic).
- **Tasks**:
  - Automate daily backups of critical directories (e.g., `/var/www`, `/etc`).
  - Sync backups to remote storage (e.g., AWS S3, from your AWS EC2 discussions).
  - Test restores weekly to ensure recoverability.
  - Rotate backups to manage disk space.
- **Commands/Tools**:
  - `rsync`: Incremental backups (`rsync -av /var/www /backup`).
  - `tar`: Archive and compress (`tar -czvf backup.tar.gz /var/www`).
  - `aws s3 sync`: Sync to S3 (`aws s3 sync /var/www s3://my-backup-bucket`).
  - `restic`: Encrypted backups (`restic backup /var/www`).
- **Example**: `tar -czvf /backup/www-$(date +%F).tar.gz /var/www` to back up web data.
- **Automation**: Python script to back up data and sync to S3.

```x-python
import subprocess
import datetime
import logging
import boto3
import os

# Configuration
LOG_FILE = "/var/log/backup.log"
SOURCE_DIR = "/var/www"
BACKUP_DIR = "/backup/www"
S3_BUCKET = "my-backup-bucket"
AWS_REGION = "us-east-1"

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")

def backup_to_local():
    """Create local compressed backup."""
    os.makedirs(BACKUP_DIR, exist_ok=True)
    backup_file = f"{BACKUP_DIR}/www-{datetime.datetime.now().strftime('%Y-%m-%d')}.tar.gz"
    try:
        subprocess.run(["tar", "-czf", backup_file, SOURCE_DIR], check=True)
        logging.info(f"Local backup created: {backup_file}")
        return backup_file
    except subprocess.CalledProcessError as e:
        logging.error(f"Local backup failed: {e}")
        return None

def sync_to_s3(backup_file):
    """Sync backup to AWS S3."""
    if not backup_file:
        return
    s3 = boto3.client("s3", region_name=AWS_REGION)
    s3_key = f"backups/www-{datetime.datetime.now().strftime('%Y-%m-%d')}.tar.gz"
    try:
        s3.upload_file(backup_file, S3_BUCKET, s3_key)
        logging.info(f"Synced to S3: s3://{S3_BUCKET}/{s3_key}")
    except Exception as e:
        logging.error(f"S3 sync failed: {e}")

def rotate_backups():
    """Remove backups older than 30 days."""
    thirty_days_ago = datetime.datetime.now() - datetime.timedelta(days=30)
    for file in os.listdir(BACKUP_DIR):
        file_path = os.path.join(BACKUP_DIR, file)
        file_mtime = datetime.datetime.fromtimestamp(os.path.getmtime(file_path))
        if file_mtime < thirty_days_ago:
            os.remove(file_path)
            logging.info(f"Removed old backup: {file_path}")

if __name__ == "__main__":
    backup_file = backup_to_local()
    sync_to_s3(backup_file)
    rotate_backups()
```

- **Script Explanation**:
  - Uses `subprocess` to create a `tar` archive of `/var/www`.
  - Syncs to AWS S3 using `boto3` (install: `sudo pip install boto3`).
  - Rotates local backups older than 30 days to manage disk space.
  - Logs actions to `/var/log/backup.log`.
  - **Execution**: `sudo python3 /usr/local/bin/backup_data.py` (requires AWS CLI credentials: `aws configure`).
  - **Schedule**: Add to `crontab -e`: `0 2 * * * python3 /usr/local/bin/backup_data.py` (runs daily at 2 AM).
- **Enterprise Impact**: Ensures data recoverability, aligns with your backup topic, and supports compliance (e.g., GDPR).

### 4. Security Hardening
**Goal**: Protect the system from vulnerabilities and unauthorized access.
- **Tasks**:
  - Apply security updates: Install Ubuntu patches daily.
  - Configure firewall: Restrict ports to necessary services.
  - Monitor logs: Detect failed login attempts or anomalies (from your logging topic).
  - Harden SSH: Disable root login, use key-based authentication.
- **Commands/Tools**:
  - `apt update && apt upgrade`: Install updates (`sudo apt update && sudo apt upgrade -y`).
  - `ufw`: Manage firewall (`sudo ufw allow 80`).
  - `fail2ban`: Prevent brute-force attacks (`sudo apt install fail2ban`).
  - `grep`: Analyze logs (`grep "Failed password" /var/log/auth.log`).
- **Example**: `sudo ufw allow 443` to open HTTPS port.
- **Automation**: Python script to check for updates and monitor SSH logs.

```x-python
import subprocess
import logging
import smtplib
from email.mime.text import MIMEText
import datetime

# Configuration
LOG_FILE = "/var/log/security_check.log"
EMAIL = "admin@example.com"
SMTP_SERVER = "smtp.example.com"
SMTP_PORT = 587
SMTP_USER = "your_smtp_user"
SMTP_PASS = "your_smtp_password"
SSH_LOG = "/var/log/auth.log"

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")

def send_alert(subject, message):
    """Send email alert."""
    msg = MIMEText(message)
    msg["Subject"] = subject
    msg["From"] = EMAIL
    msg["To"] = EMAIL
    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASS)
            server.sendmail(EMAIL, EMAIL, msg.as_string())
        logging.info(f"Alert sent: {subject}")
    except Exception as e:
        logging.error(f"Failed to send alert: {e}")

def check_updates():
    """Check and apply security updates."""
    try:
        subprocess.run(["apt", "update"], check=True)
        result = subprocess.run(["apt", "list", "--upgradable"], capture_output=True, text=True)
        if "upgradable" in result.stdout:
            subprocess.run(["apt", "upgrade", "-y"], check=True)
            logging.info("Applied security updates")
            send_alert("Security Updates Applied", "Applied security updates on {}".format(datetime.datetime.now()))
        else:
            logging.info("No updates available")
    except subprocess.CalledProcessError as e:
        logging.error(f"Update check failed: {e}")
        send_alert("Update Check Failed", f"Failed to check/apply updates: {e}")

def monitor_ssh_logs():
    """Check for failed SSH login attempts."""
    try:
        result = subprocess.run(["grep", "Failed password", SSH_LOG], capture_output=True, text=True)
        if result.stdout:
            send_alert("Failed SSH Logins Detected", f"Failed login attempts:\n{result.stdout}")
            logging.info("Detected failed SSH logins")
    except subprocess.CalledProcessError as e:
        logging.error(f"SSH log check failed: {e}")

if __name__ == "__main__":
    check_updates()
    monitor_ssh_logs()
```

- **Script Explanation**:
  - Checks for security updates using `apt` and applies them.
  - Monitors `/var/log/auth.log` for failed SSH logins.
  - Sends email alerts for updates applied or SSH failures.
  - Logs to `/var/log/security_check.log`.
  - **Execution**: `sudo python3 /usr/local/bin/security_check.py`.
  - **Schedule**: Add to `crontab -e`: `0 4 * * * python3 /usr/local/bin/security_check.py` (runs daily at 4 AM).
- **Enterprise Impact**: Reduces vulnerabilities, aligns with your security hardening practices, and supports compliance.

### 5. Log Management
**Goal**: Prevent disk space issues due to log growth (from your logging topic).
- **Tasks**:
  - Rotate logs: Use `logrotate` to manage log size.
  - Monitor log directories: Ensure `/var/log` doesnâ€™t fill up.
  - Analyze logs: Check for errors or anomalies.
- **Commands/Tools**:
  - `logrotate`: Rotate logs (`sudo logrotate -f /etc/logrotate.conf`).
  - `du -sh /var/log`: Check log directory size.
  - `journalctl`: View systemd logs (`journalctl -u nginx`).
- **Example**: Configure `/etc/logrotate.d/app`:
  ```bash
  /var/log/app/*.log {
      daily
      rotate 7
      compress
      missingok
  }
  ```
- **Automation**: Included in `backup_data.py` (rotates backups); use `logrotate` for logs.

### 6. High Availability for Large Applications
**Goal**: Ensure applications (e.g., web apps, databases) are always available.
- **Tasks**:
  - Use Docker/Kubernetes: Deploy apps in containers for scalability (from your containerization topic).
  - Configure load balancing: Use Nginx or HAProxy for traffic distribution.
  - Set up redundancy: Run multiple instances across servers or regions.
  - Monitor health checks: Ensure containers/services are healthy.
- **Commands/Tools**:
  - `docker run`: Deploy containers with health checks (`docker run --health-cmd "curl -f http://localhost/"`).
  - `kubectl`: Manage Kubernetes for HA (from your Kubernetes discussions).
  - `nginx -t`: Test Nginx configuration.
- **Example**: Run Nginx with health checks:
  ```bash
  docker run -d --name web -p 80:80 --health-cmd "curl -f http://localhost/ || exit 1" nginx
  ```
- **Automation**: Use `monitor_services.py` for containerized services; extend for Kubernetes with `kubectl`.

### 7. Disaster Recovery Planning
**Goal**: Prepare for and recover from failures.
- **Tasks**:
  - Test backups: Restore `/var/www` to `/tmp/restore` weekly.
  - Document runbook: Include steps for service restarts, restores, and failover.
  - Simulate failures: Stop services or delete containers to test recovery.
- **Commands/Tools**:
  - `tar -xzvf`: Restore backups (`tar -xzvf /backup/www-2025-07-16.tar.gz -C /tmp/restore`).
  - `docker inspect`: Check container health (`docker inspect --format '{{.State.Health.Status}}' web`).
- **Example**: Test restore: `tar -xzvf /backup/www-2025-07-16.tar.gz -C /tmp/restore`.
- **Automation**: Covered in `backup_data.py`; schedule weekly restore tests manually.

## Standard Practices
1. **Proactive Monitoring**:
   - Use `monitor_resources.py` and `monitor_services.py` to catch issues early.
   - Integrate with Prometheus/Grafana: Export metrics from `psutil` (add `prometheus_client`).
   - Set up CloudWatch for AWS-hosted servers (from your AWS EC2 discussions).

2. **Regular Updates**:
   - Run `security_check.py` daily to apply patches.
   - Monitor CVEs: Use tools like `lynis` (`sudo apt install lynis; lynis audit system`).

3. **Backup Strategy**:
   - Follow 3-2-1 rule: 3 copies, 2 media, 1 off-site (use `backup_data.py` with S3).
   - Test restores weekly to ensure recoverability.
   - Rotate backups to save space (handled in `backup_data.py`).

4. **Security Hardening**:
   - Restrict SSH: Edit `/etc/ssh/sshd_config` (`PermitRootLogin no`, `PasswordAuthentication no`).
   - Use `ufw`: Allow only necessary ports (`sudo ufw allow 80,443,22`).
   - Enable `fail2ban`: Protect against brute-force attacks.

5. **Log Management**:
   - Configure `logrotate` for all logs (e.g., `/var/log/app`, `/var/log/nginx`).
   - Monitor log size: `du -sh /var/log/*`.
   - Analyze logs: Use `grep` or ELK Stack for anomalies (from your logging topic).

6. **High Availability**:
   - Use Docker Compose or Kubernetes for multi-container apps (from your containerization topic).
   - Configure Nginx load balancer: `upstream backend { server 10.0.0.1:8080; server 10.0.0.2:8080; }`.
   - Implement failover: Use AWS Auto Scaling or Kubernetes replicas (from your Kubernetes discussions).

7. **Automation**:
   - Schedule scripts with `cron` or `systemd` timers (from your backup topic).
   - Use Ansible for multi-server management (from your automation discussions).
   - Deploy scripts via Git: Store in a repository and use CI/CD (e.g., GitHub Actions, from your Terraform topic).

8. **Documentation**:
   - Maintain a runbook: Document backup schedules, recovery steps, and monitoring alerts.
   - Use wikis (e.g., Confluence) for team access.
   - Track changes: Commit scripts to Git (`git commit -m "Add resource monitor"`).

## Troubleshooting Common Issues
1. **CPU/Memory Throttling**:
   - Check: `htop` or `monitor_resources.py` logs.
   - Fix: Kill high-usage processes (`kill -9 <pid>`) or adjust Docker limits (`docker run --cpus="1" --memory="1g"`).
   - Prevent: Set resource limits in Docker/Kubernetes.

2. **Disk Space Full**:
   - Check: `df -h /var/lib/docker`.
   - Fix: Run `docker system prune -f --volumes` or `backup_data.py` rotation.
   - Prevent: Monitor with `monitor_resources.py`.

3. **Service Downtime**:
   - Check: `systemctl status nginx` or `monitor_services.py` logs.
   - Fix: Restart services (`systemctl restart nginx`) or redeploy containers.
   - Prevent: Use health checks and redundancy.

4. **Security Breaches**:
   - Check: `security_check.py` logs or `fail2ban-client status sshd`.
   - Fix: Ban IPs with `fail2ban` or update firewall rules.
   - Prevent: Apply patches daily, use key-based SSH.

5. **Backup Failures**:
   - Check: `tail /var/log/backup.log`.
   - Fix: Verify AWS credentials (`aws configure`) or disk space.
   - Prevent: Test restores regularly.

## Practice Exercises
1. **Deploy Monitoring Scripts**:
   - Save `monitor_resources.py` and `monitor_services.py`.
   - Schedule: `crontab -e` for 15-minute and 10-minute runs.
   - Test: Simulate high CPU (`stress --cpu 4`) and verify alerts.

2. **Set Up Backups**:
   - Deploy `backup_data.py` and configure AWS S3.
   - Run: `python3 /usr/local/bin/backup_data.py`.
   - Test restore: `tar -xzvf /backup/www-$(date +%F).tar.gz -C /tmp/restore`.

3. **Harden Security**:
   - Run `security_check.py` daily.
   - Configure `ufw`: `sudo ufw allow 80,443; sudo ufw enable`.
   - Test SSH: Try logging in with password (should fail).

4. **Configure Log Rotation**:
   - Create `/etc/logrotate.d/app` (use example above).
   - Test: `sudo logrotate -f /etc/logrotate.d/app`.
   - Verify: Check `/var/log/app` for compressed logs.

5. **Simulate HA Setup**:
   - Run two Nginx containers: `docker run -d -p 8080:80 nginx`, `docker run -d -p 8081:80 nginx`.
   - Configure Nginx load balancer: Add upstream block.
   - Test: `curl http://localhost` to verify load balancing.

## Cheat Sheet
| **Task** | **Command/Script** | **Example** |
|----------|--------------------|-------------|
| Monitor CPU/Memory | `monitor_resources.py` | `python3 /usr/local/bin/monitor_resources.py` |
| Check service status | `monitor_services.py` | `python3 /usr/local/bin/monitor_services.py` |
| Backup data | `backup_data.py` | `python3 /usr/local/bin/backup_data.py` |
| Apply updates | `security_check.py` | `python3 /usr/local/bin/security_check.py` |
| Rotate logs | `logrotate` | `sudo logrotate -f /etc/logrotate.conf` |
| Check disk usage | `df -h` | `df -h /var/lib/docker` |
| Restart service | `systemctl` | `sudo systemctl restart nginx` |
| Monitor containers | `docker ps` | `docker ps -f status=running` |
| Harden firewall | `ufw` | `sudo ufw allow 80` |

## Resources
- **Man Pages**: `man systemctl`, `man docker`, `man ufw`, `man logrotate`.
- **Online**:
  - Ubuntu Server Guide (`help.ubuntu.com`): System administration.
  - Docker Docs (`docs.docker.com`): Container management.
  - AWS Docs (`aws.amazon.com/documentation`): S3 and CloudWatch.
  - Python Docs (`docs.python.org`): `psutil`, `boto3`, `smtplib`.
- **Practice**: Set up an Ubuntu VM, deploy scripts, and simulate failures (e.g., fill disk, stop services).

---

